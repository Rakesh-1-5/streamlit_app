# -*- coding: utf-8 -*-
"""Anveshan Hackathon.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c4uQG3cu1iBwDeAyFnZBpbllcqgk6Gvc
"""

pip install pandas

import pandas as pd

df=pd.read_csv('training data.csv')

df

value_missing=df.isnull().sum()

value_missing

df['location'].unique()

df['location'].value_counts()

# Drop rows with missing target
df_c = df.dropna(subset=['fraudulent'])

df_c

df.drop('fraudulent', axis=1, inplace=True)

df

# Combine text features
df['text'] = df[['title', 'company_profile', 'description', 'requirements', 'benefits']].fillna('').agg(' '.join, axis=1)

df['text']

# Drop columns not needed for now
df = df.drop(['job_id', 'telecommuting', 'has_company_logo', 'has_questions', 'title',
              'company_profile', 'description', 'requirements', 'benefits'], axis=1)

df

df.head(10)

df_fill=df.fillna('department')

df_fill

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

df = pd.read_csv("training data.csv")  # Training dataset
print("Shape:", df.shape)

# Fill missing text columns with empty string and combine
text_cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']
df[text_cols] = df[text_cols].fillna('')

# Combine into a single text column for TF-IDF
df['full_text'] = df[text_cols].agg(' '.join, axis=1)

# Ensure 'fraudulent' is binary and has no nulls
df = df.dropna(subset=['fraudulent'])
df['fraudulent'] = df['fraudulent'].astype(int)

structured_cols = ['location', 'department', 'employment_type',
                   'required_experience', 'required_education',
                   'industry', 'function']

# Fill missing with 'Unknown'
df[structured_cols] = df[structured_cols].fillna('Unknown')

# Apply Label Encoding
encoders = {}
for col in structured_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    encoders[col] = le

vectorizer = TfidfVectorizer(max_features=500)
X_text = vectorizer.fit_transform(df['full_text']).toarray()

X_structured = df[structured_cols].values
X = np.hstack((X_structured, X_text))  # Final feature matrix
y = df['fraudulent'].values            # Target variable

print("Before SMOTE:", np.bincount(y))

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

print("After SMOTE:", np.bincount(y_resampled))

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import classification_report, f1_score

X_train, X_val, y_train, y_val = train_test_split(X, y, stratify=y)

model = RandomForestClassifier(class_weight='balanced')
model.fit(X_train, y_train)
y_pred = model.predict(X_val)
print(f1_score(y_val, y_pred))

def preprocess_and_predict(new_csv_path, model, vectorizer, encoders, structured_cols, text_cols):
    # Step 1: Load new data
    new_df = pd.read_csv('/content/testing data.csv')

    # Step 2: Handle text features
    new_df[text_cols] = new_df[text_cols].fillna('')
    new_df['full_text'] = new_df[text_cols].agg(' '.join, axis=1)
    X_text = vectorizer.transform(new_df['full_text']).toarray()

    # Step 3: Handle structured features
    new_df[structured_cols] = new_df[structured_cols].fillna('Unknown')

    for col in structured_cols:
        if col in encoders:
            le = encoders[col]
            # Use existing encoder with unseen value handling
            new_df[col] = new_df[col].apply(lambda x: x if x in le.classes_ else 'Unknown')
            le_classes = np.append(le.classes_, 'Unknown') if 'Unknown' not in le.classes_ else le.classes_
            le.classes_ = le_classes
            new_df[col] = le.transform(new_df[col])

    X_structured = new_df[structured_cols].values

    # Step 4: Combine features
    X_final = np.hstack((X_structured, X_text))

    # Step 5: Predict
    y_pred = model.predict(X_final)
    y_proba = model.predict_proba(X_final)[:, 1]  # Probability of fraud (class 1)

    # Step 6: Append predictions to DataFrame
    new_df['fraudulent_predicted'] = y_pred
    new_df['fraud_probability'] = y_proba.round(4)

    return new_df[['fraudulent_predicted', 'fraud_probability'] + list(new_df.columns)]

pip install streamlit

import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

st.title("Job Scam Detector")
uploaded_file = st.file_uploader("Upload Job Listings CSV")

if uploaded_file:
    df = pd.read_csv(uploaded_file)
    processed_df = preprocess(df)
    preds = model.predict(processed_df)
    probs = model.predict_proba(processed_df)[:, 1]

    df["Fraud_Prob"] = probs
    df["Prediction"] = preds
    st.dataframe(df)

    st.subheader("Fraud Probability Histogram")
    st.bar_chart(df["Fraud_Prob"])

    st.subheader("Pie Chart: Fake vs Real")
    st.write(df["Prediction"].value_counts())

